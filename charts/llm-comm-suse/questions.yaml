questions:
# Networking Group - For configuring how services are exposed
- variable: llmChat.service.type
  group: "Networking"
  label: "LLM Chat Service Type"
  description: "Method to expose the LLM Chat service."
  type: "enum"
  required: true
  default: "NodePort"
  options:
    - "ClusterIP"
    - "NodePort"
    - "LoadBalancer"

- variable: openWebui.service.type
  group: "Networking"
  label: "Open WebUI Service Type"
  description: "Method to expose the Open WebUI service."
  type: "enum"
  required: true
  default: "NodePort"
  options:
    - "ClusterIP"
    - "NodePort"
    - "LoadBalancer"

- variable: ollama.service.type
  group: "Networking"
  label: "Ollama Service Type"
  description: "Method to expose the Ollama service. 'ClusterIP' is recommended."
  type: "enum"
  required: true
  default: "ClusterIP"
  options:
    - "ClusterIP"
    - "NodePort"
    - "LoadBalancer"

# Ollama Settings Group
- variable: ollama.hardware.type
  group: "Ollama Settings"
  label: "Target Hardware for Ollama"
  description: "Select the target hardware architecture for the Ollama deployment."
  type: "enum"
  required: true
  default: "nvidia"
  options:
    - label: "NVIDIA GPU (amd64)"
      value: "nvidia"
    - label: "Apple Silicon / CPU (arm64)"
      value: "apple"

- variable: ollama.gpu.enabled
  group: "Ollama Settings"
  label: "Enable NVIDIA GPU Acceleration"
  description: "Requires a node with NVIDIA drivers and the k8s device plugin installed."
  type: "boolean"
  default: false
  show_if: "ollama.hardware.type=nvidia"

- variable: ollama.resources.requests.cpu
  group: "Ollama Settings"
  label: "Ollama CPU Request"
  description: "The amount of CPU to reserve for the main Ollama container (e.g., '500m', '1')."
  type: "string"
  default: "2"

- variable: ollama.resources.requests.memory
  group: "Ollama Settings"
  label: "Ollama Memory Request"
  description: "The amount of memory to reserve for the main Ollama container (e.g., '1Gi', '2048Mi')."
  type: "string"
  default: "2Gi"

- variable: ollama.resources.limits.cpu
  group: "Ollama Settings"
  label: "Ollama CPU Limit"
  description: "The maximum amount of CPU the main Ollama container can use."
  type: "string"
  default: "4"

- variable: ollama.resources.limits.memory
  group: "Ollama Settings"
  label: "Ollama Memory Limit"
  description: "The maximum amount of memory the main Ollama container can use."
  type: "string"
  default: "16Gi"

# Ollama Settings Group
- variable: ollama.persistence.enabled
  group: "Ollama Settings"
  label: "Enable Ollama Model Persistence"
  description: "If true, a PersistentVolumeClaim will be created to store Ollama models."
  type: "boolean"
  default: false

- variable: ollama.persistence.size
  group: "Ollama Settings"
  label: "Ollama Model Storage Size"
  description: "The size of the PersistentVolumeClaim for Ollama models (e.g., '10Gi', '50Gi')."
  type: "string"
  default: "10Gi"
  show_if: "ollama.persistence.enabled=true"

- variable: ollama.persistence.storageClassName
  group: "Ollama Settings"
  label: "Ollama Model Storage Class Name"
  description: "The storage class name for the Ollama models PVC. Leave empty to use the default storage class."
  type: "string"
  default: ""
  show_if: "ollama.persistence.enabled=true"

# Development Settings Group
- variable: llmChat.devMode.enabled
  group: "Development Settings"
  label: "Enable LLM Chat Development Mode"
  description: "If true, the LLM Chat app will run in development mode, allowing SSH access and git pull for code updates."
  type: "boolean"
  default: false

- variable: llmChat.devMode.image.tag
  group: "Development Settings"
  label: "LLM Chat Dev Image Tag"
  description: "The container image tag for the LLM Chat App in development mode."
  type: "string"
  default: "latest-dev"
  show_if: "llmChat.devMode.enabled=true"

- variable: llmChat.devMode.persistence.enabled
  group: "Development Settings"
  label: "Enable LLM Chat Dev Code Persistence"
  description: "If true, a PersistentVolumeClaim will be created to store the LLM Chat app code in development mode."
  type: "boolean"
  default: false
  show_if: "llmChat.devMode.enabled=true"

- variable: llmChat.devMode.persistence.size
  group: "Development Settings"
  label: "LLM Chat Dev Code Storage Size"
  description: "The size of the PersistentVolumeClaim for LLM Chat app code in development mode (e.g., '1Gi', '5Gi')."
  type: "string"
  default: "1Gi"
  show_if: "llmChat.devMode.enabled=true && llmChat.devMode.persistence.enabled=true"

- variable: llmChat.devMode.persistence.storageClassName
  group: "Development Settings"
  label: "LLM Chat Dev Code Storage Class Name"
  description: "The storage class name for the LLM Chat app code PVC in development mode. Leave empty to use the default storage class."
  type: "string"
  default: ""
  show_if: "llmChat.devMode.enabled=true && llmChat.devMode.persistence.enabled=true"

- variable: llmChat.devMode.gitRepo
  group: "Development Settings"
  label: "LLM Chat Dev Git Repository"
  description: "The Git repository URL to clone for the LLM Chat app code in development mode."
  type: "string"
  default: "https://github.com/suse-technical-marketing/suse-ai-demo.git"
  show_if: "llmChat.devMode.enabled=true"

- variable: llmChat.devMode.gitBranch
  group: "Development Settings"
  label: "LLM Chat Dev Git Branch"
  description: "The Git branch to checkout for the LLM Chat app code in development mode."
  type: "string"
  default: "main"
  show_if: "llmChat.devMode.enabled=true"

# Images Group
- variable: ollama.image.tag
  group: "Images"
  label: "Ollama Image Tag"
  description: "The container image tag for Ollama."
  type: "string"
  default: "0.6.8"

- variable: openWebui.image.tag
  group: "Images"
  label: "Open WebUI Image Tag"
  description: "The container image tag for Open WebUI."
  type: "string"
  default: "main"

- variable: llmChat.image.tag
  group: "Images"
  label: "LLM Chat Image Tag"
  description: "The container image tag for the LLM Chat App."
  type: "string"
  default: "latest"
