questions:
# General Configuration
- variable: replicaCount
  label: "Replica Count"
  type: int
  description: "Number of Ollama pods for load balancing. Usually 1 for GPU workloads due to resource constraints, increase for CPU-only deployments."
  required: true
  default: 1
  group: General

- variable: image.repository
  label: "Ollama Image Repository"
  type: string
  description: "Container registry for Ollama images. Use official 'ollama/ollama' or your private registry for custom builds."
  default: "ollama/ollama"
  group: General

- variable: image.tag
  label: "Ollama Image Tag"
  type: string
  description: "Ollama version to deploy. Use stable versions like '0.3.0' for production. Check Ollama releases for GPU compatibility."
  required: true
  default: "0.3.0"
  group: General

- variable: image.pullPolicy
  label: "Image Pull Policy"
  type: enum
  description: "Policy for pulling container images"
  default: "IfNotPresent"
  group: General
  options:
    - "IfNotPresent"
    - "Always"
    - "Never"

- variable: imagePullSecrets
  label: "Image Pull Secrets"
  type: string
  description: "Comma-separated list of Kubernetes secrets for pulling private container images. Use 'application-collection' for default registry access."
  default: "application-collection"
  group: General

- variable: model
  label: "Default Model"
  type: string
  description: "LLM model to download and serve (e.g., 'llama3.2:latest', 'mistral:7b', 'codellama:13b'). Larger models need more RAM/VRAM."
  required: true
  default: "llama3.2:latest"
  group: General

# Service Configuration
- variable: service.type
  label: "Service Type"
  description: "How to expose Ollama API: 'ClusterIP' (internal only), 'NodePort' (external via node IP:port), 'LoadBalancer' (cloud LB)."
  type: enum
  required: true
  default: "ClusterIP"
  group: General
  options:
    - "ClusterIP"
    - "NodePort"
    - "LoadBalancer"

- variable: service.port
  label: "Service Port"
  type: int
  description: "TCP port for Ollama REST API. Default 11434 is the standard Ollama server port for client connections."
  required: true
  default: 11434
  group: General

# Hardware Configuration
- variable: hardware.type
  label: "Target Hardware"
  description: "Hardware platform for optimal performance: NVIDIA GPU for fastest inference, Apple Silicon for M1/M2 Macs, CPU for broad compatibility."
  type: enum
  required: true
  default: "nvidia"
  group: General
  options:
    - label: "NVIDIA GPU (amd64)"
      value: "nvidia"
    - label: "Apple Silicon / CPU (arm64)"
      value: "apple"
    - label: "Intel/AMD CPU (amd64)"
      value: "cpu"

- variable: gpu.enabled
  label: "Enable GPU Acceleration"
  description: "Use NVIDIA GPUs for faster model inference. Requires GPU-enabled Kubernetes nodes with NVIDIA device plugin installed."
  type: boolean
  default: false
  show_if: "hardware.type=nvidia"
  group: General

# Storage Configuration
- variable: persistence.enabled
  label: "Enable Model Persistence"
  description: "Store downloaded models persistently to avoid re-downloading on pod restart. Essential for production to reduce startup time."
  type: boolean
  default: true
  group: General

- variable: persistence.size
  label: "Storage Size"
  description: "Storage for model files. LLMs are large: 7B models ~4GB, 13B ~8GB, 70B ~40GB. Plan for multiple models plus growth."
  type: string
  default: "50Gi"
  show_if: "persistence.enabled=true"
  group: General

- variable: persistence.storageClassName
  label: "Storage Class"
  description: "Storage performance class. Fast SSD recommended for large models to reduce loading time. Leave empty for cluster default."
  type: string
  default: ""
  show_if: "persistence.enabled=true"
  group: General

# Resource Configuration
- variable: resources.requests.cpu
  label: "CPU Request"
  type: string
  description: "Guaranteed CPU cores for model inference. 2+ cores recommended for decent performance, more for larger models or CPU-only inference."
  default: "2"
  group: General

- variable: resources.requests.memory
  label: "Memory Request"
  type: string
  description: "Guaranteed RAM for model loading. Minimum 4Gi for 7B models, 8Gi+ for 13B, 32Gi+ for 70B models. GPU reduces RAM needs."
  default: "4Gi"
  group: General

- variable: resources.limits.cpu
  label: "CPU Limit"
  type: string
  description: "Maximum CPU cores for model inference. Higher limits improve performance for CPU-based inference and concurrent requests."
  default: "8"
  group: General

- variable: resources.limits.memory
  label: "Memory Limit"
  type: string
  description: "Maximum RAM before restart. Set higher than model size to prevent OOM kills. Include buffer for OS and inference overhead."
  default: "16Gi"
  group: General

# Advanced Configuration
- variable: additionalModels
  label: "Additional Models"
  type: string
  description: "Extra models to download at startup (comma-separated). Example: 'codellama:7b,mistral:latest'. Increases storage and startup time."
  default: ""
  group: "Advanced"

- variable: env.OLLAMA_KEEP_ALIVE
  label: "Keep Alive Duration"
  type: string
  description: "How long to keep models loaded in memory after last request (e.g., '5m', '1h', '-1' for indefinite). Longer = faster responses, more memory."
  default: "5m"
  group: "Advanced"

- variable: env.OLLAMA_NUM_PARALLEL
  label: "Parallel Requests"
  type: int
  description: "Maximum concurrent inference requests. Higher values increase throughput but require more memory. Start with 1, increase with available resources."
  default: 1
  group: "Advanced"

- variable: showAdvanced
  label: "Show Advanced Options"
  description: "Show advanced configuration options"
  type: boolean
  default: false
  group: "Advanced"